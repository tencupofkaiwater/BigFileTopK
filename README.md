# BigFileTopK
100GB url 文件，使用 1GB 内存计算出出现次数 top100 的 url 和出现的次数。

## 设计思路
假设一个特定的硬件环境：单机+单核+1G。
如果是多核，可以考虑把数据按照核数等分，然后多线程执行，然后汇总结果。

### 第1步. 拆分文件
把大文件分成若干小文件，并保持相同url在同一个文件中。每个url放在哪个文件是通过hash算出来的，我们假设拆分后的大部分文件在500M~1G之间，按照平均500M一个文件的大小，100G/500M计算出文件个数(不是1G因为内存只有1G，需要确保大部分文件都不超过1G)。某些情况下部分url重复较多，会超过1G, 这种情况，可以按照同样的方法再进行拆分成更小的文件，极限情况下一个URL重复都超过1G，做特殊处理无需再拆分。这里没有细化后面两种特殊情况，只是考虑相对正常的情况，思路是相同。

### 第2步. 统计分析
- 使用hash_map，统计每一个文件中重复URL次数；
- 为每一个hash_map创建对应的一个最小堆，堆大小保持topk+1大小，保持topk+1是为了预防删除了最小的但是在topk范围url，最终结果只取topk。
- 后一次生成的堆和前一次的堆进行合并，保持一个最大统计的堆。
- 把最终的最小堆的值一个一个取出来，应该是升序的，然后逆序打印出来就可以了。

## 实现
可以直接看源代码文件：topk.cpp。
